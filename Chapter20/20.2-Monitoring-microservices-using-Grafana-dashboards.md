<!-- MarkdownTOC -->
- [Monitoring microservices using Grafana dashboards](#monitoring-microservices-using-grafana-dashboards)
    - [Installing a local mail server for tests](#installing-a-local-mail-server-for-tests)
    - [Starting up the load test](#starting-up-the-load-test)
    - [Using Kiali's built-in Grafana dashboards](#using-kialis-built-in-grafana-dashboards)  
    - [Importing existing Grafana dashboards](#importing-existing-grafana-dashboards)
    - [Developing your own Grafana dashboards](#developing-your-own-grafana-dashboards)
        - [Examining Prometheus metrics](#examining-prometheus-metrics)
        - [Creating the dashboard](#creating-the-dashboard)
            - [Creating an empty dashboard](#creating-an-empty-dashboard)
            - [Creating a new panel for the circuit breaker metric](#creating-a-new-panel-for-the-circuit-breaker-metric)
            - [Creating a new panel for the retry metric](#creating-a-new-panel-for-the-retry-metric)
            - [Arranging the panels](#arranging-the-panels)
         - [Trying out the new dashboard](#trying-out-the-new-dashboard)
            - [Testing the circuit breaker metrics](#testing-the-circuit-breaker-metrics)
            - [Testing the retry metrics](#testing-the-retry-metrics)

<!-- /MarkdownTOC -->

# Monitoring microservices using Grafana dashboards

As we already mentioned in the introduction, Kiali is integrated with Grafana and provides some very useful dashboards out of the box. In general, they are focused on application- level performance metrics such as requests per second, response times, and fault percentages for processing requests. They are, as we will see shortly, very useful on an application level. But if we want to understand the usage of the underlying hardware resources, we need more detailed metrics, for example, Java VM-related metrics.

Grafana has an active community that, among other things, shares reusable dashboards. We will try out a dashboard from the community that's tailored for getting a lot of valuable Java VM-related metrics from a Spring Boot 2 application such as our
microservices. Finally, we will see how we can build our own dashboards in Grafana. But let's start by exploring the integration between Kiali and Grafana.

Before we do that, we need to make two preparations:

1. Install a local mail - server for tests and configure Grafana to be able to send emails to it.
We will use the mail server in the section "Setting up alarms in Grafana".

2. To be able to monitor some metrics, we will start the load test tool we used in previous chapters.

## Installing a local mail server for tests

In this section, we will set up a local test mail server and configure Grafana to send alert emails to the mail server.
Grafana can send emails to any SMPT mail server, but to keep the tests local, we will deploy a test mail server named maildev. Consider the following steps:

1. Install the test mail server with the following commands:
```
kubectl create deployment mail-server --image djfarrelly/maildev:1.1.0
kubectl expose deployment mail-server --port=80,25 --type=ClusterIP
kubectl wait --timeout=60s --for=condition=ready pod -l app=mail-server
```
2. Verify that the test mail server is up and running by visiting its web page at http://mail-server.hands-on.svc.cluster.local. Expect a web page such as the following to be rendered:

3. Configure Grafana to send emails to the test mail server by setting up a number of environment variables. Run the following commands:
```
kubectl -n istio-system set env deployment/grafana \
                GF_SMTP_ENABLED=true \
                GF_SMTP_SKIP_VERIFY=true \
                GF_SMTP_HOST=mail-server.hands-on.svc.cluster.local:25 \
                GF_SMTP_FROM_ADDRESS=grafana@minikube.me
kubectl -n istio-system wait --timeout=60s --for=condition=ready   pod -l app=grafana
```
- For more details, see https://hub.docker.com/r/djfarrelly/maildev.

Now, we have a test mail server up and running and Grafana has been configured to send emails to it. In the next section we will start the load test tool.

## Starting up the load test

To have something to monitor, let's start up the load test using Siege, which we used in previous chapters. Run the following commands:
```
ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d  grant_type=password -d username=magnus -d password=password -s | jq   .access_token -r)

siege https://minikube.me/product-composite/2 -H "Authorization: Bearer  $ACCESS_TOKEN" -c1 -d1      
```
Now, we are ready to learn about the integration between Kiali and Grafana and explore the Grafana dashboards that come with Istio.

## Using Kiali's built-in Grafana dashboards

In Chapter 18, Using a Service Mesh to Improve Observability and Management, in
the Observing the service mesh section, we learned about Kiali, but we skipped the part where Kiali shows performance metrics. Now, it's time to get back to that subject!
Execute the following steps to learn about Kiali's integration with Grafana:
1. Open the Kiali web UI in a web browser using the http://kiali.istio-system.svc.cluster.local:20001 URL. Log in with admin/admin if required.
2. Go to the service page by clicking on the Services tab from the menu on the left- hand side.
3. Select the Product service page by clicking on it.
4. On the Service:product page, select the Inbound Metrics tab. You will see the following page:
5. Kiali will visualize some overall performance graphs. However, far more detailed performance metrics are available in Grafana. Click on the View in Grafana link and Grafana will open up in a new tab. Expect a web page like the following:
6. There are a lot of performance metrics at an application-level being presented here, such as HTTP request rates, response times, and error rates. The metrics are presented for the Product service, that is, the service that was selected in Kiali. Click on the Service drop-down menu in the top left corner of the page to select another service. Feel free to look around!

Istio comes with a set of pre-installed Grafana dashboards; click on Istio/Istio Service Dashboard to view a list of available dashboards. Now, select the Istio Mesh Dashboard. You will see a web page that looks similar to the following:
This dashboard gives a very good overview of the microservices that are parts of the service mesh, as well as their current status in terms of requests per second, response times, and success rate.

As we've already mentioned, the Istio dashboards give a very good overview at an application level. But there is also a need for monitoring the metrics of hardware usage per microservice. In the next section, we will learn about how existing dashboards can be imported—specifically, a dashboard showing Java VM metrics for a Spring Boot 2-based application.

## Importing existing Grafana dashboards

As we've already mentioned, Grafana has an active community that shares reusable dashboards. They can be explored at https:/​/​grafana.​com/​grafana/​dashboards. We will try out a dashboard, called JVM (Micrometer), that's tailored for getting a lot of valuable JVM-related metrics from Spring Boot 2 applications. The URL to the dashboard is https:/ /​grafana.​com/​grafana/​dashboards/​4701. It is very easy to import a dashboard in Grafana. Perform the following steps to import this dashboard:

1. Import the dashboard named JVM (Micrometer) by following these steps: 
    1. On the Grafana web page, click on the + sign in the left-hand side menu and then select Import.
    2. On the Import page, paste the dashboard ID 4701 into the Grafana.com Dashboard field and press the Tab key to leave the field.
    3. On the Import page that will be displayed, click on the Prometheus drop-down menu and select Prometheus.
    4. Now, by clicking on the Import button, the JVM (Micrometer) dashboard will be imported and rendered.
2. Inspect the JVM (Micrometer) dashboard by following these steps:
    1. To get a good view of the metrics, click on the time picker on the top-right. This will allow you to select a proper time interval:
        1. Select Last 5 minutes as the range. Click on the time picker again and set the refresh rate to 5 seconds.
        2. Click on the Apply button after specifying the refresh rate.
    2. In the Application drop-down menu, which can be found on the top- left of the page, select the product-composite microservice.
    3. Since we are running a load test using Siege in the background, we will see a lot of metrics. The following is a sample screenshot:
    
      In this dashboard, we can find all types of Java VM relevant metrics for, among others, CPU, memory, and I/O usage, as well as HTTP-related metrics such as requests/second, average duration, and error rates. Feel free to explore these metrics on your own!
Being able to import existing dashboards is of great value when we want to get started quickly. However, what's even more important is to know how to create our own dashboard. We will learn about this in the next section.

## Developing your own Grafana dashboards

Getting started with developing Grafana dashboards is straightforward. The important thing for us to understand is what metrics Prometheus makes available for us.

In this section, we will learn how to examine the available metrics. Based on these, we will create a dashboard that can be used to monitor some of the more interesting metrics.
### Examining Prometheus metrics
In the Changes in source code for collecting application metrics section, we configured Prometheus to collect metrics from our microservices. We can actually make a call to the same endpoint and see what metrics Prometheus collects. Run the following command:
```
curl http://product-composite.hands-on.svc.cluster.local:4004/actuator/prometheus -s
```
Expect a lot of output from the command, as in the following example:

Among all of the metrics that are reported, there are two very interesting ones:
- resilience4j_retry_calls: Resilience4j reports on how its retry mechanism operates. It reports four different values for successful and failed requests, with and without retries.
- resilience4j_circuitbreaker_state: Resilience4j reports on the state of the circuit breaker.

Note that the metrics have a label named application, which contains the name of the microservice. This field comes from the configuration of the management.metrics.tags.application property, which we did in the Changes in source code for collecting application metrics section.
These metrics seem interesting to monitor. None of the dashboards we have used so far use metrics from Resilience4j. In the next section, we will create a dashboard for these metrics.

### Creating the dashboard

In this section, we will learn how to create a dashboard that visualizes the Resilience4j metrics we described in the previous section.
We will set up the dashboard in the following subsections:

#### Creating an empty dashboard

Creating a new panel for the circuit breaker metric Creating a new panel for the retry metric Arranging the panels
Creating an empty dashboard
Perform the following steps to create an empty dashboard:
1. In the Grafana web page, click on the + sign in the left-hand menu and then select dashboard.
2. A web page named New dashboard will be displayed:
3. Click on the dashboard settings button (it has a gear as its icon), as shown in the preceding screenshot. Then, follow these steps:
    1. Specify the name of the dashboard in the Name field and set the value to Hands-on Dashboard.
    2. Click on the Save button.
4. Click on the time picker to select the default values for the dashboard, as follows:
    1. Select Last 5 minutes as the range.
    2. Click on the time picker again and specify 5 seconds as the refresh rate
    in the Refreshing every field at the bottom of the panel.
    3. Click on the Apply button after specifying a refresh rate.
    4. Click on the Save button from the menu at the top of the page.
    5. Enable Save current time range and click on the Save button in the Save Changes dialog window.
     
#### Creating a new panel for the circuit breaker metric

Perform the following steps to create a new panel for the circuit breaker metric:
1. Click on the Add panel button at the top-left of the page (it has an icon of a
graph with a + sign next to it).
2. Click on the Add Query button. A page will be displayed where the new panel
can be configured.
3. In the query field, under the A letter, specify the name of the circuit breaker
metric, that is, resilience4j_circuitbreaker_state.
4. In the Legend field, specify the format, that is, {{application}}.{{namespace}}. This will create a legend in the panel where the involved microservices will be labeled with its name and namespace.
5. The filled in values should look as follows:
6. Click on the third tab, named General, from the left-hand side menu and set the Title field to Circuit Breaker.
7. Press the back button on the top-left of the page to get back to the dashboard.

#### Creating a new panel for the retry metric

Here, we will repeat the same procedure that we went through for adding a panel for the preceding circuit breaker metric, but instead, we will specify the values for the retry metrics:
1. In the query field, specify rate(resilience4j_retry_calls[30s]). Since the retry metric is a counter, its value will only go up. An ever-increasing metric is rather uninteresting to monitor. The rate function is used to convert the retry metric into a rate per second metric. The time window specified, that is, 30 s, is used by the rate function to calculate the average values of the rate.
2. For the legend, specify {{application}}.{{namespace}} ({{kind}}). Just like the output for the preceding Prometheus endpoint, we will get four metrics for the retry mechanism. To separate them in the legend, the kind attribute needs to be added.
3. Note that Grafana immediately starts to render a graph in the panel editor based on the specified values.
4. Specify Retry as the title.
5. Press the back button to get back to the dashboard.

#### Arranging the panels

Perform the following steps to arrange the panels on the dashboard:
1. You can resize a panel by dragging its lower right-hand corner to the preferred size.
2. You can also move a panel by dragging its header to the desired position.
3. The following is an example layout of the two panels:
4. Finally, click on the Save button at the top of the page. A Save Changes dialog will show up; enter an optional description and hit the Save button.

With the dashboard created we are ready to try it out: in the next section, we will try out both metrics.

### Trying out the new dashboard

Before we start testing the new dashboard, we must stop the load test tool, Siege. For this, go to the command window where Siege is running and press Ctrl + C to stop it.

Let's start by testing how to monitor the circuit breaker. Afterward, we will try out the retry metrics.

#### Testing the circuit breaker metrics
If we force the circuit breaker to open up, its state will change from closed to open, and then eventually to the half-open state. This should be reported in the circuit breaker panel.
Open the circuit, just like we did in Chapter 13, Improving Resilience Using Resilience4j, in the Trying out the circuit breaker and retry mechanism section; that is, make three requests to the API in a row, all of which will fail. Run the following commands:
```
ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)

for ((n=0; n<3; n++)); do curl -o /dev/null -skL -w "%{http_code}\n" https://minikube.me/product-composite/2?delay=3 -H "Authorization: Bearer $ACCESS_TOKEN" -s; done

```

We can expect three 500 as a response, indicating three errors in a row, that is, what it takes to open the circuit breaker!

Expect the value for the circuit breaker metric to rise to 1, indicating that the circuit is open. After a while, it should rise to 2, indicating that the circuit is now half-open. This demonstrates that we can monitor that the circuit breaker opens up if there are problems. Close the circuit breaker again by issuing three successful requests to the API with the following command:

```
for ((n=0; n<3; n++)); do curl -o /dev/null -skL -w "%{http_code}\n" https://minikube.me/product-composite/2?delay=0 -H "Authorization: Bearer $ACCESS_TOKEN" -s; done
```

We will get three 200 as responses. Note that the circuit breaker metric goes back to 0 again in the dashboard; that is, it's closed.
 At some rare occasions, I have noticed that the circuit breaker metrics are not reported in Grafana. If they don't show up after a minute, simply rerun the preceding command to reopen the circuit breaker again.

After this test, the Grafana dashboard should look as follows:

From the preceding screenshot, we can see that the retry mechanism also reports metrics that succeeded and failed.

Now that we have seen the circuit breaker metrics in action, let's see the retry metrics in action!

#### Testing the retry metrics

To trigger the retry mechanism, we will use the faultPercentage parameter we used in previous chapters. To avoid triggering the circuit breaker, we need to use relatively low values for the parameter. Run the following command:
```
while true; do curl -o /dev/null -s -L -w "%{http_code}\n" -H "Authorization: Bearer $ACCESS_TOKEN" -k https://minikube.me/product-composite/2?faultPercent=10; sleep 3; done
```

The preceding command will call the API once every third second. It specifies that 10% of the requests shall fail so that the retry mechanism kicks in and retries the failed request. After a few minutes, the dashboard should report metrics such as the following:

In the preceding screenshot, we can see that the majority of the requests have been executed successfully, without any retries. Approximately 10% of the requests have been retried by the retry mechanism and successfully executed after the retry. Before proceeding to the next section, remember to stop the request loop that we started for the preceding retry test!

In the next section, we will learn how to set up alarms in Grafana, based on these metrics.
