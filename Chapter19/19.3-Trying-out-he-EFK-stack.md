# Trying out the EFK stack
The first thing we need to do before we can try out the EFK stack is initialize Kibana so it knows what search indices to use in Elasticsearch. Once that is done, we will try out the following, in my experience, common tasks:
1. We will start by analyzing of what types of log records Fluentd has collected and stored in Elasticsearch. Kibana has a very useful visualization capability that can be used for this.
2. Next, we will learn how to discover log records from different microservices that belong to one and the same processing of an external request to the API. We will use the trace ID in the log records as a correlation ID to find related log records.
3. Thirdly, we will learn how to use Kibana to perform root cause analysis, that is, find the actual reason for an error.

## Initializing Kibana
Before we start to use Kibana, we must specify what search indices to use in Elasticsearch and what field in the indices holds the timestamps for the log records.
Perform the following steps to initialize Kibana:

1. Open Kibana's web UI using the  ***http://kibana.logging.svc.cluster.local:5601*** URL in a web browser.
2. On the welcome page, Welcome to Kibana, click on the Explore on my own button.
3. Click on the Expand button in the lower-left corner to view the names of the menu choices. These will be shown on the left-hand side.
4. Click on Discover in the menu to the left. You will be asked to define a pattern that's used by Kibana to identify what Elasticsearch indices it shall retrieve log records from.
5. Enter the logstash-* index pattern and click on Next Step.
6. On the next page, you will be asked to specify the name of the field that contains the timestamp for the log records. Click on the drop-down list for the Time Filter field name and select the only available field, @timestamp.
7. Click on the Create index pattern button.
8. Kibana will show a page that summarizes the fields that are available in the selected indices.

With Kibana initialized, we are ready to examine the log records we have collected.

## Analyzing the log records
From the deployment of Fluentd, we know that it immediately started to collect a significant number of log records. So, the first thing we need to do is get an understanding of what types of log records Fluentd has collected and stored in Elasticsearch.
We will use Kibana's visualization feature to divide the log records per Kubernetes namespace and then ask Kibana to show us how the log records are divided per type of container within each namespace. A pie chart is a suitable chart type for this type of analysis. Perform the following steps to create a pie chart:
1. In Kibana's web UI, click on Visualize in the menu to the left.
2. Click on the Create new visualization button.
3. Select Pie as the visualization type.
4. Select logstash-* as the source.
5. In the time picker (a date interval selector) above the pie chart, set a date interval of your choice (set to the last 7 days in the following screenshot). Click on its calendar icon to adjust the time interval.
 Indices are, by default, named logstash for historical reasons, even though it is Flutentd that is used for log collection.
6. Click on Add to create the first bucket, as follows:
    1. Select the bucket type, that is, Split slices.
    2. For the aggregation type, select Terms from the drop-down list.
    3. As the field, select kubernetes.namespace_name.keyword.
    4. For the size, select 10.
    5. Enable Group other values in separate bucket.
    6. Enable Show missing values.
    7. Press the Apply changes button (the blue play icon above the Bucket definition). Expect a pie chart that looks similar to the following:
      We can see that the log records are divided over the namespaces we have been working with in the previous chapters: kube-system, istio-
      system, logging, cert-manager, and our own hands-on namespace. To see what containers have created the log records divided per namespace, we need to create a second bucket.
7. Click on Add again to create a second bucket:
    1. Select the bucket type, that is, Split slices.
    2. As the sub-aggregation type, select Terms from the drop-down list.
    3. As the field, select kubernetes.container_name.keyword.
    4. For the size, select 10.
    5. Enable Group other values in separate bucket.
    6. Enable Show missing values.
    7. Press the Apply changes button again. Expect a pie chart that looks similar to the following:
    Here, we can find the log records from our microservices. Most of the log records come from the product-composite microservice.      
8. At the top of the pie chart, we have a group of log records labeled missing, that is, they neither have a Kubernetes namespace nor a container name specified. What's behind these missing log records? These log records come from processes running outside of the Kubernetes cluster in the Minikube instance and they are stored using Syslog. They can be analyzed using Syslog-specific fields, specifically the identifier field. Let's create a third bucket that divides log records based on their Syslog identifier field, if any.
9. Click on Add again to create a third bucket:
    1. Select the bucket type, that is, Split slices.
    2. As the sub-aggregation type, select Terms from the drop-down list.
    3. As the field, select SYSLOG_IDENTIFIER.keyword.
    4. Enable Group other values in separate bucket.
    5. Enable Show missing values.
    6. Press the Apply changes button and expect a pie chart that looks similar to the following:

The missing log records turn out to come from the kubelet process, which manages the node from a Kubernetes perspective, and dockerd, the Docker daemon that manages all of the containers.

Now that we have found out where the log records come from, we can start to locate the actual log records from our microservices.

## Discovering the log records from microservices
In this section, we will learn how to utilize one of the main features of centralized logging, that is, finding log records from our microservices. We will also learn how to use the trace ID in the log records to find log records from other microservices that belong to one and the same process, for example, a request to the API.

Let's start by creating some log records that we can look up with the help of Kibana. We will use the API to create a product with a unique product ID and then retrieve information about the product. After that, we can try to find the log records that were created when retrieving the product information.

The creation of log records in the microservices has updated a bit from the previous chapter so that the product composite and the three core microservices, product, recommendation, and review, all write a log record with the log level set to INFO when they begin processing a get request. Let's go over the source code that's been added to each microservice:
- Product composite microservice log creation:
```
LOG.info("Will get composite product info for product.id={}", productId);
```
- Product microservice log creation:
```
LOG.info("Will get product info for id={}", productId);
```
- Recommendation microservice log creation:
```
LOG.info("Will get recommendations for product with id={}", productId)
```
- Review microservice log creation:
```
LOG.info("Will get reviews for product with id={}", productId); 
```
For more details, see the source code in the microservices folder.

Perform the following steps to use the API to create log records and then use Kibana to look up the log records:
1. Get an access token with the following command:
```
ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq            .access_token -r)

```
2. As mentioned in the introduction to this section we will start by creating a product with a unique product ID. Create a minimalistic product (without recommendations and reviews) for "productId" :1234 by executing the following command:
```
     curl -X POST -k https://minikube.me/product-composite \
              -H "Content-Type: application/json" \
              -H "Authorization: Bearer $ACCESS_TOKEN" \
              --data '{"productId":1234,"name":"product name
            1234","weight":1234}'
```
3. Read the product with the following command:
```
curl -H "Authorization: Bearer $ACCESS_TOKEN" -k         'https://minikube.me/product-composite/1234'
```           
 Expect a response similar to the following:
 Hopefully, we got some log records created by these API calls. Let's jump over to Kibana and find out!

4. On the Kibana web page, click on the Discover menu on the left. You will see something like the following:
    On the left-top corner, we can see that Kibaba has found 326,642 log records.
    The time picker shows that they are from the last 7 days. In the histogram, we can see how the log records are spread out over time. Following that is a table showing the most recent log events that were found by the query.

5. If you want to change the time interval, you can use the time picker. Click on its calendar icon to adjust the time interval.
6. To get a better view of the content in the log records, add some fields from the log records to the table under the histogram. Select the fields from the list of available fields to the left. Scroll down until the field is found. Hold the cursor over the field and an add button will appear; click on it to add the field as a column in the table. Select the following fields, in order:
     1. spring.level, the log level
     2. kubernetes.container_name, the name of the container 
     3. spring.trace, the trace ID used for distributed tracing
     4. log, the actual log message. The web page should look something similar to the following:
     The table now contains information that is of interest regarding the log records!
     
7. To find log records from the call to the GET API, we can ask Kibana to find log records where the log field contains the text product.id=1234. This matches the log output from the product composite microservice that was shown previously. This can be done by entering log:"product.id=1234" in
the Search field and clicking on the Update button (this button can also be labeled Refresh). Expect one log record to be found:

8. Verify that the timestamp is from when you called the GET API and verify that the name of the container that created the log record is comp, that is, verify that the log record was sent by the product composite microservice.

9. Now, we want to see the related log records from the other microservices that participated in the process of returning information about the product with productId 1234, that is, finding log records with the same trace ID as that of the log record we found. To do that, place the cursor over the spring.trace field for the log record. Two small magnifying glasses will be shown to the right of the field, one with a + sign and one with a - sign. Click on the magnifying glass with the + sign to filter on the trace ID.

10. Clean the Search field so that the only search criteria is the filter of the trace field. Then, click on the Update button to see the result. Expect a response similar to the following:
We can see a lot of detailed debug and trace messages that clutter the view; let's get rid of them!

11. Place the cursor over a TRACE value and click on the magnifying glass with the - sign to filter out log records with the log level set to TRACE.

12. Repeat the preceding step for the DEBUG log record.

13. We should now be able to see the four expected log records, one for each microservice involved in the lookup of product information for the product with product ID 1234:

Also, note the filters that were applied included the trace ID but excluded log records with the log level set to DEBUG or TRACE.

Now that we know how to find the expected log records, we are ready to take the next step. This will be to learn how to find unexpected log records, that is, error messages, and how to perform root cause analysis, that is, find the reason for these error messages.

## Performing root cause analyses
One of the most important features of centralized logging is that it makes it possible to analyze errors using log records from many sources and, based on that, perform root cause analysis, that is, find the actual reason for the error message.

In this section, we will simulate an error and see how we can find information about it, all of the way down to the line of source code that caused the error in one of the microservices in the system landscape. To simulate an error, we will reuse the fault parameter we introduced in Chapter 13, Improving Resilience Using Resilience4j, in the Adding programmable delays and random errors section. We can use this to force the product microservice to throw an exception. Perform the following steps:

1. Run the following command to generate a fault in the product microservice while searching for product information on the product with product ID 666:
```
curl -H "Authorization: Bearer $ACCESS_TOKEN" -k https://minikube.me/product-composite/666?faultPercent=100
```
Expect the following error in response:
Now, we have to pretend that we have no clue about the reason for this error! Otherwise, the root cause analysis wouldn't be very exciting, right? Let's assume that we work in a support organization and have been asked to investigate some problems that just occurred while an end user tried to look up information regarding a product with product ID 666.

2. Before we start to analyze the problem, let's delete the previous search filters in the Kibana web UI so that we can start from scratch. For each filter we defined in the previous section, click on their close icon (an x) to remove them. After all of the filters have been removed, the web page should look similar to the following:

3. Start by selecting a time interval that includes the point in time when the problem occurred using the time picker. For example, search the last seven days if you know that the problem occurred within the last seven days.

4. Next, search for log records with the log level set to ERROR within this timeframe. This can be done by clicking on the spring.level field in the list
of selected fields. When you click on this field, its most commonly used values will be displayed under it. Filter on the ERROR value by clicking on its magnifier, shown with the + sign. Kibana will now show log records within the selected time frame with its log level set to ERROR, like so:

5. We can see a number of error messages related to product ID 666. The top four have the same trace ID, so this seems like a trace ID of interest to use for further investigation.

6. We can also see more error messages below the top four that seem to be related to the same error but with different trace IDs. Those are caused by the retry mechanism in the product composite microservice, that is, it retries the request a couple of times before giving up and returning an error message to the caller.

7. Filter on the trace ID of the first log record in the same way we did in the previous section.

8. Remove the filter of the ERROR log level to be able to see all of the records belonging to this trace ID. Expect Kibana to respond with a lot of log records. Look to the oldest log record, that is, the one that occurred first, that looks suspicious. For example, it may have a WARN or ERROR log level or a strange log message. The default sort order is showing the latest log record at the top, so scroll down to the end and search backward (you can also change the sort order to show the oldest log record first by clicking on the small up/down arrow next to the Time column header). The WARN log message that says Bad luck, and error occurred looks like it could be the root cause of the problem. Let's investigate it further:

9. Once a log record has been found that might be the root cause of the problem, it is of great interest to be able to find the nearby stack trace describing where exceptions were thrown in the source code. Unfortunately, the Fluentd plugin we use for collecting multiline exceptions, fluent-plugin-detect-exceptions, is unable to relate stack traces to the trace ID that was used. Therefore, stack traces will not show up in Kibana when we filter on a trace ID. Instead, we can use a feature in Kibana for finding surrounding log records that show log records that have occurred in near time to a specific log record.

10. Expand the log record that says bad luck using the arrow to the left of the log record. Detailed information about this specific log record will be revealed. There is also a link named View surrounding documents; click on it to see nearby log records. Expect a web page similar to the following:

11. The log record above the bad luck log record with the stack trace for the error message Something went wrong... looks interesting and was logged by the product microservice just two milliseconds after it logged the bad luck log record. They seem to be related! The stack trace in that log record points to line 96
in ProductServiceImpl.java. Looking in the source code (see microservices/product- service/src/main/java/se/magnus/microservices/core/product/serv ices/ProductServiceImpl.java), line 96 looks as follows:
```
            throw new RuntimeException("Something went wrong...");
```

This is the root cause of the error. We did know this in advance, but now we have seen how we can navigate to it as well.

- In this case, the problem is quite simple to resolve: simply omit the faultPercent parameter in the request to the API. In other cases, the resolution of the root cause can be much harder to figure out!

This concludes this chapter on using the EFK stack for centralized logging.
