# Trying out the EFK stack
The first thing we need to do before we can try out the EFK stack is initialize Kibana so it knows what search indices to use in Elasticsearch. Once that is done, we will try out the following, in my experience, common tasks:
1. We will start by analyzing of what types of log records Fluentd has collected and stored in Elasticsearch. Kibana has a very useful visualization capability that can be used for this.
2. Next, we will learn how to discover log records from different microservices that belong to one and the same processing of an external request to the API. We will use the trace ID in the log records as a correlation ID to find related log records.
3. Thirdly, we will learn how to use Kibana to perform root cause analysis, that is, find the actual reason for an error.

## Initializing Kibana
Before we start to use Kibana, we must specify what search indices to use in Elasticsearch and what field in the indices holds the timestamps for the log records.
Perform the following steps to initialize Kibana:

1. Open Kibana's web UI using the  ***http://kibana.logging.svc.cluster.local:5601*** URL in a web browser.
2. On the welcome page, Welcome to Kibana, click on the Explore on my own button.
3. Click on the Expand button in the lower-left corner to view the names of the menu choices. These will be shown on the left-hand side.
4. Click on Discover in the menu to the left. You will be asked to define a pattern that's used by Kibana to identify what Elasticsearch indices it shall retrieve log records from.
5. Enter the logstash-* index pattern and click on Next Step.
6. On the next page, you will be asked to specify the name of the field that contains the timestamp for the log records. Click on the drop-down list for the Time Filter field name and select the only available field, @timestamp.
7. Click on the Create index pattern button.
8. Kibana will show a page that summarizes the fields that are available in the selected indices.

With Kibana initialized, we are ready to examine the log records we have collected.

## Analyzing the log records
From the deployment of Fluentd, we know that it immediately started to collect a significant number of log records. So, the first thing we need to do is get an understanding of what types of log records Fluentd has collected and stored in Elasticsearch.
We will use Kibana's visualization feature to divide the log records per Kubernetes namespace and then ask Kibana to show us how the log records are divided per type of container within each namespace. A pie chart is a suitable chart type for this type of analysis. Perform the following steps to create a pie chart:
1. In Kibana's web UI, click on Visualize in the menu to the left.
2. Click on the Create new visualization button.
3. Select Pie as the visualization type.
4. Select logstash-* as the source.
5. In the time picker (a date interval selector) above the pie chart, set a date interval of your choice (set to the last 7 days in the following screenshot). Click on its calendar icon to adjust the time interval.
 Indices are, by default, named logstash for historical reasons, even though it is Flutentd that is used for log collection.
6. Click on Add to create the first bucket, as follows:
    1. Select the bucket type, that is, Split slices.
    2. For the aggregation type, select Terms from the drop-down list.
    3. As the field, select kubernetes.namespace_name.keyword.
    4. For the size, select 10.
    5. Enable Group other values in separate bucket.
    6. Enable Show missing values.
    7. Press the Apply changes button (the blue play icon above the Bucket definition). Expect a pie chart that looks similar to the following:
      We can see that the log records are divided over the namespaces we have been working with in the previous chapters: kube-system, istio-
      system, logging, cert-manager, and our own hands-on namespace. To see what containers have created the log records divided per namespace, we need to create a second bucket.
7. Click on Add again to create a second bucket:
    1. Select the bucket type, that is, Split slices.
    2. As the sub-aggregation type, select Terms from the drop-down list.
    3. As the field, select kubernetes.container_name.keyword.
    4. For the size, select 10.
    5. Enable Group other values in separate bucket.
    6. Enable Show missing values.
    7. Press the Apply changes button again. Expect a pie chart that looks similar to the following:
    Here, we can find the log records from our microservices. Most of the log records come from the product-composite microservice.      
8. At the top of the pie chart, we have a group of log records labeled missing, that is, they neither have a Kubernetes namespace nor a container name specified. What's behind these missing log records? These log records come from processes running outside of the Kubernetes cluster in the Minikube instance and they are stored using Syslog. They can be analyzed using Syslog-specific fields, specifically the identifier field. Let's create a third bucket that divides log records based on their Syslog identifier field, if any.
9. Click on Add again to create a third bucket:
    1. Select the bucket type, that is, Split slices.
    2. As the sub-aggregation type, select Terms from the drop-down list.
    3. As the field, select SYSLOG_IDENTIFIER.keyword.
    4. Enable Group other values in separate bucket.
    5. Enable Show missing values.
    6. Press the Apply changes button and expect a pie chart that looks similar to the following:

The missing log records turn out to come from the kubelet process, which manages the node from a Kubernetes perspective, and dockerd, the Docker daemon that manages all of the containers.

Now that we have found out where the log records come from, we can start to locate the actual log records from our microservices.

## Discovering the log records from microservices
In this section, we will learn how to utilize one of the main features of centralized logging, that is, finding log records from our microservices. We will also learn how to use the trace ID in the log records to find log records from other microservices that belong to one and the same process, for example, a request to the API.

Let's start by creating some log records that we can look up with the help of Kibana. We will use the API to create a product with a unique product ID and then retrieve information about the product. After that, we can try to find the log records that were created when retrieving the product information.

The creation of log records in the microservices has updated a bit from the previous chapter so that the product composite and the three core microservices, product, recommendation, and review, all write a log record with the log level set to INFO when they begin processing a get request. Let's go over the source code that's been added to each microservice:
- Product composite microservice log creation:
```
LOG.info("Will get composite product info for product.id={}", productId);
```
- Product microservice log creation:
```
LOG.info("Will get product info for id={}", productId);
```
- Recommendation microservice log creation:
```
LOG.info("Will get recommendations for product with id={}", productId)
```
- Review microservice log creation:
```
LOG.info("Will get reviews for product with id={}", productId); 
```
For more details, see the source code in the microservices folder.

Perform the following steps to use the API to create log records and then use Kibana to look up the log records:
1. Get an access token with the following command:
```
ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq            .access_token -r)

```
2. As mentioned in the introduction to this section we will start by creating a product with a unique product ID. Create a minimalistic product (without recommendations and reviews) for "productId" :1234 by executing the following command:
```
     curl -X POST -k https://minikube.me/product-composite \
              -H "Content-Type: application/json" \
              -H "Authorization: Bearer $ACCESS_TOKEN" \
              --data '{"productId":1234,"name":"product name
            1234","weight":1234}'
```
3. Read the product with the following command:
```
curl -H "Authorization: Bearer $ACCESS_TOKEN" -k         'https://minikube.me/product-composite/1234'
```           
 Expect a response similar to the following:
 Hopefully, we got some log records created by these API calls. Let's jump over to Kibana and find out!

4. On the Kibana web page, click on the Discover menu on the left. You will see something like the following:
    On the left-top corner, we can see that Kibaba has found 326,642 log records.
    The time picker shows that they are from the last 7 days. In the histogram, we can see how the log records are spread out over time. Following that is a table showing the most recent log events that were found by the query.

5. If you want to change the time interval, you can use the time picker. Click on its calendar icon to adjust the time interval.
6. To get a better view of the content in the log records, add some fields from the log records to the table under the histogram. Select the fields from the list of available fields to the left. Scroll down until the field is found. Hold the cursor over the field and an add button will appear; click on it to add the field as a column in the table. Select the following fields, in order:
     1. spring.level, the log level
     2. kubernetes.container_name, the name of the container 
     3. spring.trace, the trace ID used for distributed tracing
     4. log, the actual log message. The web page should look something similar to the following:
     The table now contains information that is of interest regarding the log records!
     
7. To find log records from the call to the GET API, we can ask Kibana to find log records where the log field contains the text product.id=1234. This matches the log output from the product composite microservice that was shown previously. This can be done by entering log:"product.id=1234" in
the Search field and clicking on the Update button (this button can also be labeled Refresh). Expect one log record to be found:

8. Verify that the timestamp is from when you called the GET API and verify that the name of the container that created the log record is comp, that is, verify that the log record was sent by the product composite microservice.

9. Now, we want to see the related log records from the other microservices that participated in the process of returning information about the product with productId 1234, that is, finding log records with the same trace ID as that of the log record we found. To do that, place the cursor over the spring.trace field for the log record. Two small magnifying glasses will be shown to the right of the field, one with a + sign and one with a - sign. Click on the magnifying glass with the + sign to filter on the trace ID.

10. Clean the Search field so that the only search criteria is the filter of the trace field. Then, click on the Update button to see the result. Expect a response similar to the following:
We can see a lot of detailed debug and trace messages that clutter the view; let's get rid of them!

11. Place the cursor over a TRACE value and click on the magnifying glass with the - sign to filter out log records with the log level set to TRACE.

12. Repeat the preceding step for the DEBUG log record.

13. We should now be able to see the four expected log records, one for each microservice involved in the lookup of product information for the product with product ID 1234:

Also, note the filters that were applied included the trace ID but excluded log records with the log level set to DEBUG or TRACE.

Now that we know how to find the expected log records, we are ready to take the next step. This will be to learn how to find unexpected log records, that is, error messages, and how to perform root cause analysis, that is, find the reason for these error messages.
