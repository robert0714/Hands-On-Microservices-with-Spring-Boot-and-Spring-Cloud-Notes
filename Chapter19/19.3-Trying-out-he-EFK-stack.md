# Trying out the EFK stack
The first thing we need to do before we can try out the EFK stack is initialize Kibana so it knows what search indices to use in Elasticsearch. Once that is done, we will try out the following, in my experience, common tasks:
1. We will start by analyzing of what types of log records Fluentd has collected and stored in Elasticsearch. Kibana has a very useful visualization capability that can be used for this.
2. Next, we will learn how to discover log records from different microservices that belong to one and the same processing of an external request to the API. We will use the trace ID in the log records as a correlation ID to find related log records.
3. Thirdly, we will learn how to use Kibana to perform root cause analysis, that is, find the actual reason for an error.

## Initializing Kibana
Before we start to use Kibana, we must specify what search indices to use in Elasticsearch and what field in the indices holds the timestamps for the log records.
Perform the following steps to initialize Kibana:

1. Open Kibana's web UI using the  ***http://kibana.logging.svc.cluster.local:5601*** URL in a web browser.
2. On the welcome page, Welcome to Kibana, click on the Explore on my own button.
3. Click on the Expand button in the lower-left corner to view the names of the menu choices. These will be shown on the left-hand side.
4. Click on Discover in the menu to the left. You will be asked to define a pattern that's used by Kibana to identify what Elasticsearch indices it shall retrieve log records from.
5. Enter the logstash-* index pattern and click on Next Step.
6. On the next page, you will be asked to specify the name of the field that contains the timestamp for the log records. Click on the drop-down list for the Time Filter field name and select the only available field, @timestamp.
7. Click on the Create index pattern button.
8. Kibana will show a page that summarizes the fields that are available in the selected indices.

With Kibana initialized, we are ready to examine the log records we have collected.

## Analyzing the log records
From the deployment of Fluentd, we know that it immediately started to collect a significant number of log records. So, the first thing we need to do is get an understanding of what types of log records Fluentd has collected and stored in Elasticsearch.
We will use Kibana's visualization feature to divide the log records per Kubernetes namespace and then ask Kibana to show us how the log records are divided per type of container within each namespace. A pie chart is a suitable chart type for this type of analysis. Perform the following steps to create a pie chart:
1. In Kibana's web UI, click on Visualize in the menu to the left.
2. Click on the Create new visualization button.
3. Select Pie as the visualization type.
4. Select logstash-* as the source.
5. In the time picker (a date interval selector) above the pie chart, set a date interval of your choice (set to the last 7 days in the following screenshot). Click on its calendar icon to adjust the time interval.
 Indices are, by default, named logstash for historical reasons, even though it is Flutentd that is used for log collection.
6. Click on Add to create the first bucket, as follows:
    1. Select the bucket type, that is, Split slices.
    2. For the aggregation type, select Terms from the drop-down list.
    3. As the field, select kubernetes.namespace_name.keyword.
    4. For the size, select 10.
    5. Enable Group other values in separate bucket.
    6. Enable Show missing values.
    7. Press the Apply changes button (the blue play icon above the Bucket definition). Expect a pie chart that looks similar to the following:
      We can see that the log records are divided over the namespaces we have been working with in the previous chapters: kube-system, istio-
      system, logging, cert-manager, and our own hands-on namespace. To see what containers have created the log records divided per namespace, we need to create a second bucket.
7. Click on Add again to create a second bucket:
    1. Select the bucket type, that is, Split slices.
    2. As the sub-aggregation type, select Terms from the drop-down list.
    3. As the field, select kubernetes.container_name.keyword.
    4. For the size, select 10.
    5. Enable Group other values in separate bucket.
    6. Enable Show missing values.
    7. Press the Apply changes button again. Expect a pie chart that looks similar to the following:
    Here, we can find the log records from our microservices. Most of the log records come from the product-composite microservice.      
8. At the top of the pie chart, we have a group of log records labeled missing, that is, they neither have a Kubernetes namespace nor a container name specified. What's behind these missing log records? These log records come from processes running outside of the Kubernetes cluster in the Minikube instance and they are stored using Syslog. They can be analyzed using Syslog-specific fields, specifically the identifier field. Let's create a third bucket that divides log records based on their Syslog identifier field, if any.
9. Click on Add again to create a third bucket:
  1. Select the bucket type, that is, Split slices.
  2. As the sub-aggregation type, select Terms from the drop-down list.
  3. As the field, select SYSLOG_IDENTIFIER.keyword.
  4. Enable Group other values in separate bucket.
  5. Enable Show missing values.
  6. Press the Apply changes button and expect a pie chart that looks similar to the following:

The missing log records turn out to come from the kubelet process, which manages the node from a Kubernetes perspective, and dockerd, the Docker daemon that manages all of the containers.

Now that we have found out where the log records come from, we can start to locate the actual log records from our microservices.


