# Deploying the EFK stack on Kubernetes
Deploying the EFK stack on Kubernetes will be done in the same way as we have deployed our own microservices: using Kubernetes definition files for objects such as deployments, services, and configuration maps.

The deployment of the EFK stack is divided into two parts:
- One part where we deploy Elasticsearch and Kibana
- One part where we deploy Fluentd

But first, we need to build and deploy our own microservices.

## Building and deploying our microservices
Building, deploying, and verifying the deployment using the test-em-all.bash test script is done in the same way as it was done in Chapter 18, Using a Service Mesh to Improve Observability and Management, in the Running commands to create the service mesh section. Run the following commands to get started:
1. First, build the Docker images from the source with the following commands:
```
   cd $BOOK_HOME/Chapter19
   eval $(minikube docker-env)
   ./gradlew build && docker-compose build
```   
2. Recreate the namespace, hands-on, and set it as the default namespace:
```
            kubectl delete namespace hands-on
            kubectl create namespace hands-on
            kubectl config set-context $(kubectl config current-context) --namespace=hands-on
```
3. Execute the deployment by running the deploy-dev-env.bash script with the following command:
```
./kubernetes/scripts/deploy-dev-env.bash
```

4. Start the Minikube tunnel, if it's not already running (see Chapter 18, Using a Service Mesh to Improve Observability and Management, the Setting up access to Istio services section for a recap, if required):
```
   minikube tunnel
```         
    Remember that this command requires that your user has sudo privileges and that you enter your password during startup and shutdown. It takes a couple of seconds before the command asks for the password, so it is easy to miss!
5. Run the normal tests to verify the deployment with the following command:
```
   ./test-em-all.bash
```         
    Expect the output to be similar to what we have seen from the previous chapters:

6. You can also try out the APIs manually by running the following commands:
```
ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq .access_token -r)

curl -ks https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" | jq .productId
```
  Expect the requested product ID, 2, in the response.

With the microservices deployed, we can move on and deploy Elasticsearch and Kibana!

## Deploying Elasticsearch and Kibana
We will deploy Elasticsearch and Kibana to its own namespace, logging. Both Elasticsearch and Kibana will be deployed for development and test usage using a Kubernetes deployment object. This will be done with a single pod and a Kubernetes node port service. The services will expose the standard ports for Elasticsearch and
Kibana internally in the Kubernetes cluster, that is, port 9200 for Elasticserach and port 5601 for Kibana. Thanks to the minikube tunnel command, we will be able to access these services locally using the following URLs:

- elasticsearch.logging.svc.cluster.local:9200 for Elasticserch 
- kibana.logging.svc.cluster.local:5601 for Kibana

We will use the versions that were available when this chapter was written:
- Elasticsearch version 7.3.0 
- Kibana version 7.3.0

Before we perform the deployments, let's look at the most interesting parts of the definition files.


### A walkthrough of the definition files

The definition file for Elasticsearch, kubernetes/efk/elasticsearch.yml, contains a standard Kubernetes deployment and service object that we have seen multiples times before, for example, in Chapter 15, Introduction to Kubernetes, in the Trying out a sample deployment section. The most interesting part, as we explained previously, of
the definition file is the following:
```
   apiVersion: extensions/v1beta1
   kind: Deployment
   ...
         containers:
         - name: elasticsearch
           image: docker.elastic.co/elasticsearch/elasticsearch-oss:7.3.0
           resources:
             limits:
               cpu: 500m
               memory: 2Gi
             requests:
               cpu: 500m
               memory: 2Gi
```
Let's explain the preceding source code in detail:
- We use an official Docker image from Elastic that's available at docker.elastic.co with a package that only contains open source components. This is ensured by using the -oss suffix on the name of the Docker image, elasticsearch-oss. The version is set to 7.3.0.
- The Elasticsearch container is allowed to allocate a relatively large amount of memory—2 GB—to be able to perform queries with good performance. The more memory, the better the performance.
 
The definition file for Kibana, kubernetes/efk/kibana.yml, also contains a standard Kubernetes deployment and service object. The most interesting parts in the definition file are as follows:
```
   apiVersion: extensions/v1beta1
   kind: Deployment
   ...
         containers:
         - name: kibana
           image: docker.elastic.co/kibana/kibana-oss:7.3.0
           env:
           - name: ELASTICSEARCH_URL
             value: http://elasticsearch:9200
```

Let's explain the preceding source code in detail:
- For Kibana, we also use an official Docker image from Elastic that's available at docker.elastic.co, along with a package that only contains open source components, kibana-oss. The version is set to 7.3.0.
- To connect Kibana with the Elasticsearch pod, an environment variable, ELASTICSEARCH_URL, is defined to specify the address to the Elasticsearch service, http://elasticsearch:9200.

With these insights, we are ready to perform the deployment of Elasticsearch and Kibana.

### Running the deploy commands
Deploy Elasticsearch and Kibana by performing the following steps:

1. Create a namespace for Elasticsearch and Kibana with the following command:
```
kubectl create namespace logging
```
2. To make the deploy steps run faster, prefetch the Docker images for Elasticsearch and Kibana with the following commands:
```
            eval $(minikube docker-env)
            docker pull docker.elastic.co/elasticsearch/elasticsearch-oss:7.3.0
            docker pull docker.elastic.co/kibana/kibana-oss:7.3.0
```
3. Deploy Elasticsearch and wait for its pod to be ready with the following commands:
```
            kubectl apply -f kubernetes/efk/elasticsearch.yml -n logging
            kubectl wait --timeout=120s --for=condition=Ready pod -n logging --all
```

4. Verify that Elasticsearch is up and running with the following command:
```
            curl http://elasticsearch.logging.svc.cluster.local:9200 -s | jq -r  .tagline
```
Expect You Know, for Search as a response.

5. Deploy Kibana and wait for its pod to be ready with the following commands:
```
            kubectl apply -f kubernetes/efk/kibana.yml -n logging
            kubectl wait --timeout=120s --for=condition=Ready pod -n logging --all
```
- Depending on your hardware, you might need to wait for a minute or two before Elasticsearch responds with this message.

6. Verify that Kibana is up and running with the following command:
```
            curl -o /dev/null -s -L -w "%{http_code}\n"    http://kibana.logging.svc.cluster.local:5601
```
  Expect 200 as the response.

With Elasticsearch and Kibana deployed, we can start to deploy Fluentd.

## Deploying Fluentd

Deploying Fluentd is a bit more complex compared to deploying Elasticsearch and Kibana. To deploy Fluentd, we will use a Docker image that's been published by the Fluentd project on Docker Hub, fluent/fluentd-kubernetes-daemonset, and sample the Kubernetes definition files from a Fluentd project on GitHub, fluentd-kubernetes-daemonset. It is located at https:/​/​github.​com/​fluent/​fluentd-​kubernetes-​daemonset. As it's implied by the name of the project, Fluentd will be deployed as a daemon set, running one pod per node in the Kubernetes cluster. Each Fluentd pod is responsible for collecting log output from processes and containers that run on the same node as the pod. Since we are using Minikube, that is, a single node cluster, we will only have one Fluentd pod.

To handle multiline log records that contain stack traces from exceptions, we will use a third-party Fluentd plugin provided by Google, fluent-plugin-detect-exceptions, which is available at https:/​/​github.​com/​GoogleCloudPlatform/​fluent-​plugin-​detect- exceptions. To be able to use this plugin, we will build our own Docker image where the fluent-plugin-detect-exceptions plugin will be installed. Fluentd's Docker
image, fluentd-kubernetes-daemonset, will be used as the base image. We will use the versions that were available when this chapter was written:
- Fluentd version 1.4.2 
- fluent-plugin-detect-exceptions version 0.0.12

Before we perform the deployments, let's look at the most interesting parts of the definition files.

### A walkthrough of the definition files
The Dockerfile that's used to build the Docker image, kubernetes/efk/Dockerfile, looks as follows:
```Dockerfile
   FROM fluent/fluentd-kubernetes-daemonset:v1.4.2-debian-elasticsearch-1.1
   RUN gem install fluent-plugin-detect-exceptions -v 0.0.12 \
    && gem sources --clear-all \
    && rm -rf /var/lib/apt/lists/* \
              /home/fluent/.gem/ruby/2.3.0/cache/*.gem
```

Let's explain the preceding source code in detail:
- The base image is Fluentd's Docker image, fluentd-kubernetes-daemonset. The v1.4.2-debian-elasticsearch-1.1 tag specifies that version v1.4.2 shall be used with a package that contains built-in support for sending log records to Elasticsearch. The base Docker image contains the Fluentd configuration files that were mentioned in the Configuring Fluentd section.
- The Google plugin, fluent-plugin-detect-exceptions, is installed using Ruby's package manager, gem.

The definition file of the daemon set, kubernetes/efk/fluentd-ds.yml, is based on a sample definition file in the fluentd-kubernetes-daemonset project, which can be found at https:/​/​github.​com/​fluent/​fluentd-​kubernetes-​daemonset/​blob/​master/ fluentd-​daemonset-​elasticsearch.​yaml. This file is a bit complex, so let's go through the most interesting parts separately:
1. First, here's the declaration of the daemon set:
```
            apiVersion: extensions/v1beta1
            kind: DaemonSet
            metadata:
              name: fluentd
              namespace: kube-system
```
Let's explain the preceding source code in detail:
- The kind key specifies that this is a daemon set.
- The namespace key specifies that the daemon set shall be created in the kube-system namespace and not in the logging namespace where Elasticseach and Kibana are deployed.
2. The next part specifies the template for the pods that are created by the daemon set. The most interesting parts are as follows:
```
           spec:
             template:
               spec:
                 containers:
                 - name: fluentd
                   image: hands-on/fluentd:v1
                   env:
                     - name: FLUENT_ELASTICSEARCH_HOST
                       value: "elasticsearch.logging"
                     - name: FLUENT_ELASTICSEARCH_PORT
                       value: "9200"
```
Let's explain the preceding source code in detail:
    - The Docker image that's used for the pods is hands- on/fluentd:v1. We will build this Docker image after walking through the definition files using the Dockerfile we described previously.
    - A number of environment variables are supported by the Docker image and are used to customize it. The two most important ones are as follows:
        - FLUENT_ELASTICSEARCH_HOST, which specifies the hostname of the Elasticsearch service, that is, elasticsearch.logging
        - FLUENT_ELASTICSEARCH_PORT, which specifies the port that's used to communicate with Elasticsearch, that is, 9200
        
- Since the Fluentd pod runs in another namespace than Elasticsearch, the hostname cannot be specified using its short name, that is, elasticsearch. Instead, the namespace part of the DNS name must also be specified, that is, elasticsearch.logging. As an alternative, the fully qualified domain name (FQDN), elasticsearch.logging.svc.cluster.local, can also be used. But since the last part of the DNS name, svc.cluster.local, is shared by all DNS names inside a Kubernetes cluster, it does not need to be specified.

3. Finally, a number of volumes, that is, filesystems, are mapped into the pod, as follows:
```
                   volumeMounts:
                   - name: varlog
                     mountPath: /var/log
                   - name: varlibdockercontainers
                     mountPath: /var/lib/docker/containers
                     readOnly: true
                   - name: journal
                     mountPath: /var/log/journal
                     readOnly: true
                   - name: fluentd-extra-config
                     mountPath: /fluentd/etc/conf.d
                 volumes:
                 - name: varlog
                   hostPath:
                     path: /var/log
                 - name: varlibdockercontainers
                   hostPath:
                     path: /var/lib/docker/containers
                 - name: journal
                   hostPath:
                     path: /run/log/journal
                 - name: fluentd-extra-config
                   configMap:
                     name: "fluentd-hands-on-config"
```                     
Let's explain the preceding source code in detail:
- Three folders on the host (that is, the node) are mapped into the Fluentd pod. These folders contain the log files that Fluentd will tail and collect log records from. The folders are: /var/log, /var/lib/docker/containers and /run/log/journal.
- Our own configuration file that specifies how Fluentd shall process log records from our microservices is mapped using a config map called fluentd-hands-on-config to the /fluentd/etc/conf.d folder. The base Docker image that's used for preceding Fluentd, fluentd-kubernetes-daemonset, configures Fluentd to include any configuration file that's found in the /fluentd/etc/conf.d folder. See the Configuring
Fluentd section for details.

For the full source code of the definition file for the daemon set, see the kubernetes/efk/fluentd-ds.yml file.

Now that we've walked through everything, we are ready to perform the deployment of Fluentd.

### Running the deploy commands
To deploy Fluentd, we have to build the Docker image, create the config map, and finally deploy the daemon set. Run the following commands to perform these steps:
1. Build the Docker image and tag it with hands-on/fluentd:v1 using the following command:
```
            eval $(minikube docker-env)
            docker build -f kubernetes/efk/Dockerfile -t hands-on/fluentd:v1 kubernetes/efk/
```
2. Create the config map, deploy Fluentd's daemon set, and wait for the pod to be ready with the following commands:
```
            kubectl apply -f kubernetes/efk/fluentd-hands-on-configmap.yml
            kubectl apply -f kubernetes/efk/fluentd-ds.yml
            kubectl wait --timeout=120s --for=condition=Ready pod -l app=fluentd -n kube-system
```          
3. Verify that the Fluentd pod is healthy with the following command:
```
            kubectl logs -n kube-system $(kubectl get pod -l app=fluentd -n
            kube-system -o jsonpath={.items..metadata.name}) | grep "fluentd worker is now running worker"
```         
Expect a response of 2019-08-16 15:11:33 +0000 [info]: #0 fluentd worker is now running worker=0.

4. Fluentd will start to collect a considerable amount of log records from the various processes and containers in the Minkube instance. After a minute or so, you can ask Elasticsearch how many log records have been collected with the following command:
``` 
            curl http://elasticsearch.logging.svc.cluster.local:9200/_all/_count
```

The command can be a bit slow the first time it is executed, but should return a response similar to the following:

In this example, Elasticsearch contains 144750 log records.

This completes the deployment of the EFK stack. Now, it's time to try it out and find out

what all of the collected log records are about!
