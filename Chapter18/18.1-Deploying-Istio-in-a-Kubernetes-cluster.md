# Deploying Istio in a Kubernetes cluster
## Setting up access to Istio services
### Minikube
[reference](https://argoproj.github.io/argo-rollouts/getting-started/setup/)

1.Istio needs 8192MB of memory,4 CPUs .Start minikube with 16384 MB of memory and 4 CPUs.
[for istio 1.2.x](https://istio.io/latest/news/releases/1.2.x/announcing-1.2.10/)[k8s1.12.x](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.15.md)
```
minikube -p lab  start --memory=10240mb --cpus=4 --disk-size=30g --kubernetes-version=v1.15.12
```
- see the [compatibility](https://istio.io/v1.7/about/supported-releases/#support-status-of-istio-releases)

[for istio 1.6.x + ](https://istio.io/latest/news/releases/1.9.x/announcing-1.9.3/) [k8s1.16+](https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG)
```
minikube -p lab  start --memory=10240mb --cpus=4 --disk-size=30g  --kubernetes-version=v1.18.18
```

- [Adding files](https://minikube.sigs.k8s.io/docs/handbook/filesync/)   
    Place files to be synced in $MINIKUBE_HOME/files   
    For example, running the following will result in the deployment of a custom /etc/resolv.conf:
    ```
    mkdir -p ~/.minikube/files/etc
    echo search cluster.local > ~/.minikube/files/etc/resolv.conf
    echo nameserver 10.96.0.10 > ~/.minikube/files/etc/resolv.conf
    ```   

2. Install istio by yaml
    - creating istio-system namepsace 
    ```bash
    $ kubectl create namespace istio-system
    ```
    - 賦予istio-system namespace的預設服務帳號擁有cluster-admin權限。
    ```bash
    $ kubectl create clusterrolebinding   istio-system-cluster-rolebinding  --clusterrole=cluster-admin  --serviceaccount=istio-system:default
    ```

    - Install Istio-specific custom resource definitions (CRDs) in Kubernetes:
    ```bash
    $ ISTIO_VERSION=1.5.10
    
    $env:Path += "; $HOME\istio-$ISTIO_VERSION\bin" # or  $env:Path += ";$PWD\bin"    
    $ export PATH="$PWD/istio-${ISTIO_VERSION}/bin:$PATH"
    
    $ cd istio-$ISTIO_VERSION
    $ for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done
    ```
    -  Install Istio demo configurations in Kubernetes as follows:
    ```bash
    $ kubectl apply -f install/kubernetes/istio-demo.yaml
    ```
    -  Wait for the Istio deployments to become available:
    ```
    kubectl -n istio-system wait --timeout=600s --for=condition=available deployment --all
    ```
    The command will report deployment resources in Istio as available, one after another. Expect 12 messages such as ***deployment.extensions/NNN condition met*** before the command ends. It can take a couple of minutes (or more) depending on your hardware and internet connectivity.
    -  Update ***Kiali's config map*** with URLs to Jaeger and Grafana with the following commands:   
       ( location: Chapter18/kubernetes/istio/setup/kiali-configmap.yml )
    ```bash
        kubectl -n istio-system apply -f kubernetes/istio/setup/kiali-configmap.yml && \
        kubectl -n istio-system delete pod -l app=kiali && \
        kubectl -n istio-system wait --timeout=60s --for=condition=ready pod -l app=kiali
    ``` 
    - If you use isitio 1.4.x, you would encounter error : ***signing key for login tokens is invalid*** ,we need to modify exist .
    ```
    kubectl -n istio-system  get configmap kiali -o yaml   
    ```
    - TO edit Chapter18/kubernetes/istio/setup/kiali-configmap.yml     to reserve the key .
    ```
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: kiali
      labels:
        app: kiali
        chart: kiali
        heritage: Tiller
        release: istio
    data:
      config.yaml: |
        istio_namespace: istio-system
        deployment:
          accessible_namespaces: ['**']
        login_token:
          signing_key: \"fNQQoBXWmV\"
        server:
          port: 20001
          web_root: /kiali
        external_services:
          tracing:
            url: http://jaeger-query.istio-system.svc.cluster.local:16686/jaeger
          grafana:
            url: http://grafana.istio-system.svc.cluster.local:3000
          prometheus:
            url: http://prometheus.istio-system.svc.cluster.local:9090
    ```
    The config map, ***kubernetes/istio/setup/kiali-configmap.yml***, contains URLs to Jaeger and Grafana that utilize the DNS names set up by the minikube tunnel command used in the next section.
    
    Istio is now deployed in Kubernetes, but before we move on and create the service mesh,we need to learn a bit about how to access Istio services in a Minikube environment.
    
or by minikube addons
```
minikube -p lab  addons enable istio-provisioner
minikube -p lab  addons enable istio
```
or  by [ istioctl](https://istio.io/latest/docs/setup/install/istioctl/)

```
$ istioctl install
```
or  by [ github releases](https://github.com/istio/istio/releases)
osx or linux 
```
$ curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.9.2 sh -
```
refer [this](https://istio.io/latest/docs/setup/getting-started/)   
```
$ cd istio-1.9.2
$ export PATH=$PWD/bin:$PATH
$ istioctl install --set profile=demo -y
✔ Istio core installed
✔ Istiod installed
✔ Egress gateways installed
✔ Ingress gateways installed
✔ Installation complete

$ kubectl label namespace default istio-injection=enabled
```
- Deploy prometheus   
    https://istio.io/latest/docs/ops/integrations/prometheus/
- Deploy kialai   
    https://istio.io/latest/docs/ops/integrations/kiali/
- Deploy grafana   
    https://istio.io/latest/docs/ops/integrations/grafana/
- Deploy jaeger     
    https://istio.io/latest/docs/ops/integrations/jaeger/
- Deploy the sample application   
  1. Deploy the [Bookinfo sample application](https://istio.io/latest/docs/examples/bookinfo/):
    ```
    $ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
      service/details created
      serviceaccount/bookinfo-details created
      deployment.apps/details-v1 created
      service/ratings created
      serviceaccount/bookinfo-ratings created
      deployment.apps/ratings-v1 created
      service/reviews created
      serviceaccount/bookinfo-reviews created
      deployment.apps/reviews-v1 created
      deployment.apps/reviews-v2 created
      deployment.apps/reviews-v3 created
      service/productpage created
      serviceaccount/bookinfo-productpage created
      deployment.apps/productpage-v1 created
    ```
  2. The application will start. As each pod becomes ready, the Istio sidecar will be deployed along with it.
    ```
    $ kubectl get services
      NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
      details       ClusterIP   10.0.0.212      <none>        9080/TCP   29s
      kubernetes    ClusterIP   10.0.0.1        <none>        443/TCP    25m
      productpage   ClusterIP   10.0.0.57       <none>        9080/TCP   28s
      ratings       ClusterIP   10.0.0.33       <none>        9080/TCP   29s
      reviews       ClusterIP   10.0.0.28       <none>        9080/TCP   29s
    ```
    and
    ```
    $ kubectl get pods
      NAME                              READY   STATUS    RESTARTS   AGE
      details-v1-558b8b4b76-2llld       2/2     Running   0          2m41s
      productpage-v1-6987489c74-lpkgl   2/2     Running   0          2m40s
      ratings-v1-7dc98c7588-vzftc       2/2     Running   0          2m41s
      reviews-v1-7f99cc4496-gdxfn       2/2     Running   0          2m41s
      reviews-v2-7d79d5bd5d-8zzqd       2/2     Running   0          2m41s
      reviews-v3-7dbcdcbc56-m8dph       2/2     Running   0          2m41s
    ```
    Re-run the previous command and wait until all pods report READY 2/2 and STATUS Running before you go to the next step. This might take a few minutes depending on your platform.
    
  3. Verify everything is working correctly up to this point. Run this command to see if the app is running inside the cluster and serving HTML pages by checking for the page title in the response:
    ```bash
    $ kubectl exec "$(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')" -c ratings -- curl -sS productpage:9080/productpage | grep -o "<title>.*</title>"
        <title>Simple Bookstore App</title>
    ```
    https://istio.io/latest/docs/setup/getting-started/
   
If you don’t have enough RAM allocated to the minikube virtual machine, the following errors could occur:
-  image pull failures
-  healthcheck timeout failures
-  kubectl failures on the host
-  general network instability of the virtual machine and the host
-  complete lock-up of the virtual machine
-  host NMI watchdog reboots
 
      One effective way to monitor memory usage in minikube is to ssh into the minikube virtual machine and from that prompt run the top command:
    ```
    $ minikube ssh

    $ top
    GiB Mem : 12.4/15.7
    ```
    This shows 12.4GiB used of an available 15.7 GiB RAM within the virtual machine. This data was generated with the VMWare Fusion hypervisor on a Macbook Pro 13” with 16GiB RAM running Istio 1.2 with bookinfo installed.

3. Label the default namespace to enable istio sidecar injection for the namespace
```
kubectl label namespace default istio-injection=enabled
```
## An added bonus from using the minikube tunnel command
[reference](https://istio.io/latest/docs/setup/platform-setup/minikube/)

(Optional, recommended) If you want minikube to provide a load balancer for use by Istio, you can use the minikube tunnel feature. Run this command in a different terminal, because the minikube tunnel feature will block your terminal (if os is windows , you need to ***Run as Administator***) to output diagnostic information about the network:
```
$ minikube -p lab tunnel
```

While running minikube tunnel, the istio-ingressgateway Service will now have an external IP which can be retrieved via kubectl:
```bash
$ kubectl get svc -n istio-system istio-ingressgateway
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                            AGE
istio-ingressgateway   LoadBalancer   10.100.136.45   10.100.136.45   15020:31711/TCP,80:31298/TCP....   7d22h

```
You can use minikube service.
```
$ minikube -p lab -n  istio-system service  istio-ingressgateway
|--------------|----------------------|-------------------|-----------------------------|
|  NAMESPACE   |         NAME         |    TARGET PORT    |             URL             |
|--------------|----------------------|-------------------|-----------------------------|
| istio-system | istio-ingressgateway | status-port/15020 | http://192.168.99.179:32759 |
|              |                      | http2/80          | http://192.168.99.179:30681 |
|              |                      | https/443         | http://192.168.99.179:30081 |
|              |                      | kiali/15029       | http://192.168.99.179:30503 |
|              |                      | prometheus/15030  | http://192.168.99.179:30002 |
|              |                      | grafana/15031     | http://192.168.99.179:32179 |
|              |                      | tracing/15032     | http://192.168.99.179:30719 |
|              |                      | tls/15443         | http://192.168.99.179:30935 |
|--------------|----------------------|-------------------|-----------------------------|
* Opening service istio-system/istio-ingressgateway in default browser...
* Opening service istio-system/istio-ingressgateway in default browser...
* Opening service istio-system/istio-ingressgateway in default browser...
* Opening service istio-system/istio-ingressgateway in default browser...
* Opening service istio-system/istio-ingressgateway in default browser...
* Opening service istio-system/istio-ingressgateway in default browser...
* Opening service istio-system/istio-ingressgateway in default browser...
* Opening service istio-system/istio-ingressgateway in default browser...

```


Sometimes minikube does not clean up the tunnel network properly. To force a proper cleanup:
```
$ minikube tunnel --cleanup
```
### workaround by other methods
#### Install keepalived loadbalancer 
helm v2 to install under k8s1.16+
```
$  minikube -p sample start
$  kubectl label node sample [default:minikube] proxy=true
$  kubectl create clusterrolebinding \
   keepalived-cluster-role-binding \
   --clusterrole=cluster-admin --serviceaccount=keepalived:default
$  kubectl create namespace keepalived
$  git clone https://github.com/robert0714/keepalived.git 
$  cd keepalived && git  checkout k8s-1.16  && cd ..
$  helm install keepalived --name keepalived \
   --namespace keepalived \
   --set keepalivedCloudProvider.serviceIPRange="192.168.99.150/29" \
   --set nameOverride="lb"
```
helm v3
```
 helm install keepalived  keepalived \
   --namespace keepalived \
   --set keepalivedCloudProvider.serviceIPRange="192.168.99.150/29" \
   --set nameOverride="lb"
```
to make sure :
```bash
$ kubectl get svc -n istio-system istio-ingressgateway
NAME                   TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                                                                                                                      AGE
istio-ingressgateway   LoadBalancer   10.111.84.81   192.168.99.145   15020:32759/TCP,80:30681/TCP,443:30081/TCP,15029:30503/TCP,15030:30002/TCP,15031:32179/TCP,15032:30719/TCP,15443:30935/TCP   9m28s
```
#### Mappins the DNS name to the external Ip
Config minikube.me to be resolved to the IP address of the Istio Ingress Gateway as follows:
1. Get the IP address exposed by the keepalived loadbalancer for the Istio Ingress Gateway and save it in an environment variable named INGRESS_HOST:
```bash
INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
INGRESS_HOST=$(minikube -p lab ip)
```
2. Update /etc/hosts so that minikube.me points to the Istio Ingress Gateway:
```
echo "$INGRESS_HOST minikube.me" | sudo tee -a /etc/hosts
```
If os is windows ,the file is C:\Windows\System32\drivers\etc\hosts. use administrative Powershell with the following command:
```
Add-Content -Path $env:windir\System32\drivers\etc\hosts -Value "`n$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')`tminikube.me" -Force
```

3. Remove the line in /etc/hosts where minikube.me that points to the IP address of the Minikube instance (minikube ip). Verify that /etc/hosts only contains one line that translates minikube.me and that it points to the IP address of the Istio Ingress Gateway.

### Verify that Kiali, Jaeger, and Grafana
Verify that Kiali, Jaeger, and Grafana can be reached through the tunnel with the following commands:
```bash
curl -o /dev/null -s -L -w "%{http_code}" http://kiali.istio-system.svc.cluster.local:20001/kiali/
curl -o /dev/null -s -L -w "%{http_code}" http://grafana.istio-system.svc.cluster.local:3000
curl -o /dev/null -s -L -w "%{http_code}" http://jaeger-query.istio-system.svc.cluster.local:16686
```
[notice](https://minikube.sigs.k8s.io/docs/handbook/accessing/)    
Minikube's DNS resolution  is experimental 

If you are on ***macOS with hyperkit as VM driver***, the tunnel command also allows DNS resolution for Kubernetes services from the host.

NOTE: docker driver doesn’t support DNS resolution

Each command should return 200 (OK ) on ***macOS with hyperkit as VM driver***

[debug reference](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/)   
Use that manifest to create a Pod:
```
kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml
```
Once that Pod is running, you can exec nslookup in that environment. If you see something like the following, DNS is working correctly.
```
kubectl exec -i -t dnsutils -- nslookup kubernetes.default
```
Take a look inside the resolv.conf file. (See Inheriting DNS from the node and Known issues below for more information)
```
kubectl exec -ti dnsutils -- cat /etc/resolv.conf
```
Use the kubectl get pods command to verify that the DNS pod is running.
```
kubectl get pods --namespace=kube-system -l k8s-app=kube-dns
```
Use the kubectl logs command to see logs for the DNS containers.

For CoreDNS:
```
kubectl logs --namespace=kube-system -l k8s-app=kube-dns
```
Verify that the DNS service is up by using the kubectl get service command.
```
kubectl get svc --namespace=kube-system
```
You can verify that DNS endpoints are exposed by using the kubectl get endpoints command.
```
kubectl get endpoints kube-dns --namespace=kube-system
```
You can verify if queries are being received by CoreDNS by adding the log plugin to the CoreDNS configuration (aka Corefile). The CoreDNS Corefile is held in a ConfigMap named coredns. To edit it, use the command:
```
kubectl -n kube-system edit configmap coredns
```
- Are you in the right namespace for the service?  
 
    DNS queries that don't specify a namespace are limited to the pod's namespace.

    If the namespace of the pod and service differ, the DNS query must include the namespace of the service.

    This query is limited to the pod's namespace:
    ```
    kubectl exec -i -t dnsutils -- nslookup <service-name>
    ```
    This query specifies the namespace:
    ```
    kubectl exec -i -t dnsutils -- nslookup <service-name>.<namespace>
    ```
## An added bonus from using the minikube tunnel command
Running the **minikube tunnel** command also makes it possible to access some other cluster-internal Kubernetes services that may be of interest. Once the environment is up and running as described in the ***Running commands to create the service mesh*** section, the following can be achieved:
- The ***health*** endpoint of the ***product-composite*** microservice can be checked with the following command:
```
curl -k http://product-composite.hands-on.svc.cluster.local:4004/actuator/health
```
   Refer to the ***Observing the service mesh*** section for an explanation of the use of port 4004.
   
- MySQL tables in the review database can be accessed with the following command:
```
mysql -umysql-user-dev -pmysql-pwd-dev review-db -e "select * from reviews" -h mysql.hands-on.svc.cluster.local
```

- MongoDB collections in the product and recommendations databases can be accessed with the following commands:
```
mongo --host mongodb.hands-on.svc.cluster.local -u mongodb-user-dev -p mongodb-pwd-dev --authenticationDatabase admin product-db --eval "db.products.find()"

mongo --host mongodb.hands-on.svc.cluster.local -u mongodb-user-dev -p mongodb-pwd-dev --authenticationDatabase admin recommendation-db --eval "db.recommendations.find()"
```

- RabbitMQ's web UI can be accessed using the following
URL: ***http://rabbitmq.hands-on.svc.cluster.local:15672***. Log in using the credentials ***rabbit-user-dev*** and ***rabbit-pwd-dev***.

With the Minikube tunnel in place, we are now ready to create the service mesh.
