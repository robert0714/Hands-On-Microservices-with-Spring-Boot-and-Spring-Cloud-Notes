# Performing zero-downtime deployments

## Source code changes
### Service and deployment objects for concurrent versions of microservices
To be able to run multiple versions of a microservice concurrently, the deployment objects and their corresponding pods must have different names, for example, ***product-v1*** and ***product-v2***. There must, however, be only one Kubernetes service object per microservice. All traffic to a specific microservice always goes through one and the same service object, irrespective of what version of the pod the request will be routed to in the end. This is achieved using Kustomize by splitting up deployment objects and service objects into different folders.   Details regarding the development environment have been removed from the preceding diagram.

To give deployment objects and their pods version-dependent names, the ***kustomization.yml*** file can use the nameSuffix directive to tell Kustomize to add the given suffix to all Kubernetes objects it creates. For example, the kustomization.yml file used for the v1 version of the microservices in
the ***kubernetes/services/overlays/prod/v1*** folder appears as follows:
```bash
   nameSuffix: -v1
   bases:
   - ../../../base/deployments
   patchesStrategicMerge:
   - ...
```

The ***nameSuffix: -v1*** setting will result in all objects created using this ***kustomization.yml*** file being named with the -v1 suffix.

To create the objects without a version suffix, and the deployment objects and their pods with the v1 and v2 version suffixes, the kubernetes/scripts/deploy-prod- env.bash deployment script executes separate kubectl apply commands as follows:
```bash
   kubectl apply -k kubernetes/services/base/services
   kubectl apply -k kubernetes/services/overlays/prod/v1
   kubectl apply -k kubernetes/services/overlays/prod/v2
```
Let's also see what Istio definition files we have added to configure routing rules.

### Added Kubernetes definition files for Istio
To configure routing rules, we will add Istio objects
to the ***kubernetes/services/overlays/prod/istio*** folder. Each microservice has a virtual service object that defines the weight distribution for the routing between the old and the new versions. Initially, it is set to route 100% of the traffic to the old version. For example, the routing rule for the product microservice in ***product-routing-virtual-service.yml*** appears as follows:
```
     http:
     - route:
       - destination:
           host: product
           subset: old
         weight: 100
       - destination:
           host: product
           subset: new
         weight: 0
```

The virtual service defines subsets for the old and the new versions. To define what actual versions the old and new versions are, each microservice also has a destination rule defined. The destination rule details how the old subset and the new subset shall be identified, for example, in the case of the product microservice in ***old_new_subsets_destination_rules.yml***:
```yaml
   apiVersion: networking.istio.io/v1alpha3
   kind: DestinationRule
   metadata:
     name: product-dr
   spec:
     host: product
     subsets:
     - name: old
       labels:
         version: v1
     - name: new
       labels:
         version: v2
```
The subset named ***old*** points to product pods that have the ***version*** label set to ***v1***, while the subset named new points to pods with the ***version*** label set to ***v2***.
- To route traffic to a specific version, Istio documentation recommends that pods are labeled with a label named version to identify its version. Refer to https://istio.io/docs/setup/kubernetes/additional-setup/requirements/ for details.

Finally, to support canary testers, an extra routing rule has been added to the virtual services for the three core microservices: product, recommendation, and review. This routing rule states that any incoming request that has an HTTP header named X-group set to the value test will always be routed to the new version of the service. This appears as follows:
```
     http:
     - match:
       - headers:
           X-group:
             exact: test
       route:
       - destination:
           host: product
           subset: new
```

The match and route sections specify that requests with the HTTP header, X-group, set to the value, test, shall be routed to the subset named new.

To create these Istio objects, the kubernetes/scripts/deploy-prod- env.bash deployment script executes the following command:
```
   kubectl apply -k kubernetes/services/overlays/prod/istio
```
Finally, to be able to route canary testers to the new version based on header-based routing, the product-composite microservice has been updated to forward the HTTP header X- group. Refer to the getCompositeProduct() method in the se.magnus.microservices.composite.product.services.ProductCompositeServ iceImpl class for details.

Now, we have seen all the changes to the source code and we are ready to deploy v1 and v2 versions of the microservices.

## Deploying v1 and v2 versions of the microservices with routing to the v1 version

To be able to test the v1 and v2 versions of the microservices, we need to remove the development environment we have been using earlier in this chapter and create a production environment where we can deploy the v1 and v2 versions of the microservices.

To achieve this, run the following commands: 

1. Recreate the hands-on namespace:
```
   kubectl delete namespace hands-on
   kubectl create namespace hands-on
```
2. Execute the deployment by running the script with the following command:
```
            ./kubernetes/scripts/deploy-prod-env.bash
``` 
    The command takes a couple of minutes and should eventually list all the v1 or v2 versions of the pods as follows:
3. Run the usual tests to verify that everything works:
```
            SKIP_CB_TESTS=true ./test-em-all.bash
```
If this command is executed immediately after the deploy command, it sometimes fails. Simply rerun the command and it should run fine!
- Since we now have two pods (version V1 and V2) running for each microservice, the circuit breaker tests no longer work. The reason for this is that the test script can't control which pod it talks to, through the Kubernetes service. The test script asks about the state of the circuit breaker in the product-composite microservice using the actuator endpoint on port 4004. This port is not managed by Istio, so its routing rules do no apply. The test script will therefore not know whether it is checking the state of the circuit breaker in V1 or V2 of the product- composite microservice. We can skip circuit breaker tests by using the SKIP_CB_TESTS=true flag.

Expect output that is similar to what we have seen from the previous chapters, but excluding the circuit breaker tests:

We are now ready to run some zero-downtime deploy tests. Let's begin by verifying that all traffic goes to the v1 version of the microservices!## Verifying that all traffic initially goes to the v1 version of the microservices

## Verifying that all traffic initially goes to the v1 version of the microservices

To verify that all requests are routed to the v1 version of the microservices, we will start up the load test tool, siege, and then observe the traffic that flows through the service mesh using Kiali.

Perform the following steps:
1. Get a new access token and start the siege load test tool, with the following commands:
```
ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me/oauth/token -d grant_type=password -d username=magnus -d password=password -s | jq  .access_token -r)
 
siege https://minikube.me/product-composite/2 -H "Authorization: Bearer $ACCESS_TOKEN" -c1 -d1
```

2. Go to the graph view in Kiali's web UI (http://kiali.istio-system.svc.cluster.local:20001/kiali):
    1. Click on the Display menu button and deselect Service Nodes. 
    2. After a minute or two, expect only traffic to the v1 version of the microservices as follows:

Good! This means that, even though the v2 versions of the microservices are deployed, they do not get any traffic routed to them. Let's now try out canary tests where selected test users are allowed to try out the v2 versions of the microservices!

## Running canary tests
To run a canary test, in other words, in order to be routed to the new versions while all other users are still routed to the old versions of the deployed microservices, we need to add the X-group HTTP header set to the value test in our requests sent to the external API.

To see which version of a microservice served a request, the serviceAddresses field in the response can be inspected. The serviceAddresses field contains the hostname of each service that took part in creating the response. The hostname is equal to the name of the pod, so we can find the version in the hostname; for example, product-v1-... for a product service of version V1, and product-v2-... for a product service of version V2.

Let's begin by sending a normal request and verify that it is the v1 versions of the microservices that respond to our request. Next, send a request with the X-group HTTP header set to the value test, and verify that the new v2 versions are responding.

To do this, perform the following steps:
1. Perform a normal request to verify that the request is routed to the v1 version of the microservices by using jq to filter out the serviceAddresses field in the response:
```
curl -ks https://minikube.me/product-composite/2 -H "Authorization:  Bearer $ACCESS_TOKEN" | jq .serviceAddresses
```
Expect a response along the lines of the following:
```
```
    As expected, all three core services are v1 versions of the microservices.
2. If we add the X-group=test header, we expect the request to be served by v2 versions of the core microservices. Run the following command:
```
curl -ks https://minikube.me/product-composite/2 -H "Authorization:  Bearer $ACCESS_TOKEN" -H "X-group: test" | jq .serviceAddresses
```
Expect a response similar to the following:

As expected, all three core microservices that respond are now v2 versions; that is, as a canary tester, we are routed to the new v2 versions!

Given that the canary tests returned the expected results, we are ready to allow normal users to be routed to the new v2 versions using blue/green deployment.

## Running blue/green tests
To route parts of the normal users to the new v2 versions of the microservices, we have to modify the weight distribution in the virtual services. They are currently 100/0; in other words, all traffic is routed to the old v1 versions. We can achieve this as we did before, that is, by editing the definition files of the virtual services in the kubernetes/services/overlays/prod/istio folder and then running a kubectl apply command to make the change take effect. As an alternative, we can use the kubectl patch command to change the weight distribution directly on the virtual service objects in the Kubernetes API server.

I find the patch command useful when making a number of changes to the same objects to try something out, for example, to change the weight distribution in the routing rules. In this section, we will use the kubectl patch command to quickly change
the weight distribution in the routing rules between the v1 and v2 versions of the microservices. To get the state of a virtual service after a number of kubectl patch commands have been executed, a command such as kubectl get vs NNN -o yaml can be issued. For example, to get the state of the virtual service of the product microservice, issue the following command: kubectl get vs product-vs -o yaml.

Since we haven't used the kubectl patch command before and it can be a bit involved to start with, let's undertake a short introduction to how it works before we perform the green/blue deploy.

### A short introduction to the kubectl patch command

The kubectl patch command can be used to update specific fields in an existing object in the Kubernetes API server. We will try the patch command on the virtual service for the review microservice, named review-vs. The relevant parts of the definition for the virtual service, review-vs, appear as follows:
```
spec: 
  http:
    - match: 
      ...
    - route:
      - destination:
          host: review
          subset: old
        weight: 100
      - destination:
          host: review
          subset: new
        weight: 0
```
For the full source code, refer to kubernetes/services/overlays/prod/istio/review-routing-virtual- service.yml.

A sample patch command that changes the weight distribution of the routing to the v1 and v2 pods in the review microservice appears as follows:
```
   kubectl patch virtualservice review-vs --type=json -p='[    \
     {"op": "add", "path": "/spec/http/1/route/0/weight", "value": "80"},  \
     {"op": "add", "path": "/spec/http/1/route/1/weight", "value": "20"}   \
]'
```
The command will configure the routing rules of the review microservice to route 80% of the requests to the old version, and 20% of the requests to the new version.

To specify that the weight value shall be changed in the review-vs virtual service, the /spec/http/1/route/0/weight path is given for the old version
and /spec/http/1/route/1/weight for the new version.

The 0 and 1 in the path are used to specify the index of array elements in the definition of the virtual service. For example, http/1 means the second element in the array under the http element. See the definition of the preceding review-vs virtual service.

From the preceding definition we can see that the second element is the route element. The first element with index 0 being the match element.

Now that we know a bit more about the kubectl patch command, we are ready to test a blue/green deployment.

### Performing the blue/green deployment

Now, it is time to gradually move more and more users to the new versions using blue/green deployment. To perform the deployment, run the following steps:

1. Ensure that the load test tool, Siege, is still running.
It was started in the preceding Verifying that all traffic initially goes to the v1 version of the microservices section.

2. To allow 20% of the users to be routed to the new v2 version of the review microservice, we can patch the virtual service and change weights with the following command:
```
kubectl patch virtualservice review-vs --type=json -p='[
        {"op": "add", "path": "/spec/http/1/route/0/weight", "value":"80"},
        {"op": "add", "path": "/spec/http/1/route/1/weight", "value":"20"}
]'
```

3. To observe the change in the routing rule, go to the Kiali web UI (http://kiali.istio-system.svc.cluster.local:20001/kiali) and select the graph view.

4. Change the edge label to Requests percentage.

5. Wait for a minute before the statics are updated in Kiali so that we can observe the change. Expect the graph in Kiali to show something like the following:

Depending on how long you have waited, the graph might look a bit different!
In the screenshot, we can see that Istio now routes traffic to both the v1 and v2 versions of the review microservice.

Of the 33% of the traffic that is sent to the review microservice from the product- composite microservice, 7% are routed to the new v2 pod, and 26% to the old v1 pod. This means that 7/33 (= 21%) of the requests are routed to the v2 pod, and 26/33 (= 79%) to the v1 pod. This is in line with the 20/80 distribution we have requested:

  1. Please feel free to try out the preceding kubectl patch command to affect the routing rules for the other core microservices: product and recommendation.

  2. If you want to route all traffic to the v2 version of all microservices, you can run the following script:
  ```
  ./kubernetes/scripts/route-all-traffic-to-v2-services.bash
  ```
  You have to give Kiali a minute or two to collect metrics before it can visualize the changes in routing between the v1 and v2 versions of the microservices, but remember that the change in the actual routing is immediate!
  Expect only v2 versions of the microservices to show up in the graph after a while:

  Depending on how long you have waited, the graph might look a bit different!

  3. If something goes terribly wrong after the upgrade to v2, the following command can be executed to revert all traffic back to the v1 version of all microservices:
  ```
  ./kubernetes/scripts/route-all-traffic-to-v1-services.bash
  ```
After a short while, the graph in Kiali should look like the screenshot in the previous Verifying that all traffic initially goes to the v1 version of the microservices section; that is, visualize that all requests go to the v1 version of all microservices again.

This concludes the introduction to the service mesh concept and Istio as an implementation of the concept.

Before we wrap up the chapter, let's recap how we can run tests in Docker Compose to ensure that the source code of our microservices does not rely on deployment in Kubernetes.
